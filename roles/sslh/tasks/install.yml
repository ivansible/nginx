---
- name: install sslh v1.18 from bionic on xenial
  ## with original xenial's sslh 1.17 an attempt of ssh transparent
  ## proxying for a local process (e.g. shadowsocks) fails with error
  ## "bind:98:Address already in use"
  ## issue description: https://github.com/yrutschle/sslh/issues/64
  ## fixed in sslh 1.18 by PR https://github.com/yrutschle/sslh/pull/69
  ansible.builtin.apt:
    deb: "{{ sslh_bionic_deb }}"
  when: ansible_lsb.codename in ['xenial']

- name: install sslh v1.20 from focal on bionic
  ansible.builtin.apt:
    deb: "{{ sslh_focal_deb }}"
  when: ansible_lsb.codename in ['bionic', 'eoan']

- name: install native sslh package on focal+
  ansible.builtin.apt:
    name: sslh
    state: present
  when: ansible_lsb.release is version('20.04','>=')

- name: remove apache2 packages pulled by sslh on bionic+
  ansible.builtin.apt:
    name: apache2
    state: absent
    purge: true
    autoremove: true
  when: ansible_lsb.release is version('16.04','>')

- name: modify /etc/hosts
  ansible.builtin.import_tasks: hosts.yml
  tags: etc_hosts

- name: create sslh config file
  ansible.builtin.template:
    src: sslh.cfg
    dest: /etc/sslh.cfg
    owner: root
    group: sslh
    mode: 0640
  notify: restart sslh service


- name: configure ufw firewall for sslh
  ansible.builtin.include_tasks: ufw.yml
  args:
    apply:
      become: true
      tags: lin_sslh_firewall
  when: lin_firewall == 'ufw'
  tags: lin_sslh_firewall

- name: configure ferm firewall for sslh
  ansible.builtin.include_tasks: ferm.yml
  args:
    apply:
      become: true
      tags: lin_sslh_firewall
  when: lin_firewall == 'ferm'
  tags: lin_sslh_firewall


- name: configure sslh service and helper parameters
  ansible.builtin.template:
    src: sslh.defaults
    dest: /etc/default/sslh
    owner: root
    group: sslh
    mode: 0640
  notify: restart sslh service
  tags: lin_sslh_firewall

- name: override sslh service config
  ansible.builtin.template:
    src: sslh.service
    dest: /etc/systemd/system/sslh.service
    owner: root
    group: sslh
    mode: 0644
  register: sslh_service_config_result
  notify: restart sslh service
  tags: lin_sslh_firewall

- name: enable and start sslh service (will be rescued if it fails)
  ansible.builtin.systemd:
    name: sslh
    state: started
    enabled: true
    daemon_reload: "{{ sslh_service_config_result is changed }}"
  register: sslh_service_start_result


- name: determine whether nginx service runs (and possibly blocks ssl port)
  ansible.builtin.service_facts:
  ## note: condition "ActiveState == 'failed'" is not enough
  when: sslh_service_start_result.status.ActiveState != 'active'

- name: completely disable apache2 service to unblock ssl port
  ansible.builtin.systemd:
    name: apache2
    state: stop
    enabled: false
  failed_when: false
  when:
    - ansible_facts.services.apache2 is defined
    - ansible_facts.services.apache2.state == 'running'

- name: let sslh service start when nginx blocks ssl port due to stale settings
  block:
    - name: prevent pending sslh restart before nginx gets reconfigured
      ansible.builtin.meta: flush_handlers

    - name: stop nginx to unblock ssl port (will restart after reconfiguration)
      ansible.builtin.systemd:
        name: nginx
        state: stopped
      register: nginx_service_stop_result
      ## make few attempts until nginx really stops
      until: nginx_service_stop_result.status.ActiveState != 'active'
      notify: restart nginx service
      ## sometimes change is not detected but we know for sure nginx was running
      changed_when: true

    - name: pause a little to let ssl port go free in background
      ansible.builtin.pause:
        seconds: 3

    - name: start sslh service once again after stopping nginx
      ansible.builtin.systemd:
        name: sslh
        ## perform restart instead of start to override stuck service state
        state: started
        enabled: true

  when:
    - sslh_service_start_result.status.ActiveState != 'active'
    - ansible_facts.services.nginx is defined
    - ansible_facts.services.nginx.state == 'running'
...
