---
- name: install sslh package 1.18 from bionic on xenial
  # with original xenial's sslh 1.17 an attempt of ssh transparent
  # proxying for a local process (e.g. shadowsocks) fails with error
  # "bind:98:Address already in use"
  # issue description: https://github.com/yrutschle/sslh/issues/64
  # fixed in sslh 1.18 by PR https://github.com/yrutschle/sslh/pull/69
  apt:
    deb: http://mirrors.kernel.org/ubuntu/pool/universe/s/sslh/sslh_1.18-1_amd64.deb
  when: ansible_lsb.codename == 'xenial'

- name: install sslh package on bionic+
  apt:
    name: sslh
    state: present
  when: ansible_lsb.codename == 'bionic'

- name: remove apache2 packages pulled by sslh on bionic
  apt:
    name: apache2
    state: absent
    purge: true
    autoremove: true
  when: ansible_lsb.codename == 'bionic'

- name: add dual ipv4/ipv6 localhost in /etc/hosts as sslh target
  blockinfile:
    path: /etc/hosts
    block: |
      127.0.0.1 localhost4or6
      ::1       localhost4or6
    marker: "# {mark} dual-stack target for sslh"
    # docker does not allow atomic updates of /etc/hosts
    # see https://github.com/ansible/ansible/issues/13981
    # see https://docs.docker.com/network/bridge/#differences-between-user-defined-bridges-and-the-default-bridge
    unsafe_writes: "{{ ansible_virtualization_type == 'docker' }}"
  notify: restart sslh service

- name: create sslh config file
  template:
    src: sslh.cfg
    dest: /etc/sslh.cfg
    owner: root
    group: sslh
    mode: 0640
  notify: restart sslh service


- name: configure ufw firewall for sslh
  include_tasks: ufw.yml
  args:
    apply:
      become: true
      tags: lin_sslh_firewall
  when: lin_firewall == 'ufw'
  tags: lin_sslh_firewall

- name: configure ferm firewall for sslh
  include_tasks: ferm.yml
  args:
    apply:
      become: true
      tags: lin_sslh_firewall
  when: lin_firewall == 'ferm'
  tags: lin_sslh_firewall


- name: configure sslh service and helper parameters
  template:
    src: sslh.defaults
    dest: /etc/default/sslh
    owner: root
    group: sslh
    mode: 0640
  notify: restart sslh service
  tags: lin_sslh_firewall

- name: override sslh service config
  template:
    src: sslh.service
    dest: /etc/systemd/system/sslh.service
    owner: root
    group: sslh
    mode: 0644
  register: sslh_service_config_result
  notify: restart sslh service
  tags: lin_sslh_firewall

- name: enable and start sslh service (will be rescued if it fails)
  systemd:
    name: sslh
    state: started
    enabled: true
    daemon_reload: "{{ sslh_service_config_result is changed }}"
  register: sslh_service_start_result


- name: determine whether nginx service runs (and possibly blocks ssl port)
  service_facts:
  # note: condition "ActiveState == 'failed'" is not enough
  when: sslh_service_start_result.status.ActiveState != 'active'

- name: completely disable apache2 service to unblock ssl port
  systemd:
    name: apache2
    state: stop
    enabled: false
  ignore_errors: true
  when: ansible_facts.services.apache2 is defined
        and ansible_facts.services.apache2.state == 'running'

- name: let sslh service start when nginx blocks ssl port due to stale settings
  block:
    - name: prevent pending sslh restart before nginx gets reconfigured
      meta: flush_handlers

    - name: stop nginx to unblock ssl port (will restart after reconfiguration)
      systemd:
        name: nginx
        state: stopped
      register: nginx_service_stop_result
      # make few attempts until nginx really stops
      until: nginx_service_stop_result.status.ActiveState != 'active'
      notify: restart nginx service
      # sometimes change is not detected but we know for sure nginx was running
      changed_when: true

    - name: pause a little to let ssl port go free in background
      pause:
        seconds: 3

    - name: start sslh service once again after stopping nginx
      systemd:
        name: sslh
        # perform restart instead of start to override stuck service state
        state: started
        enabled: true

  when: sslh_service_start_result.status.ActiveState != 'active'
        and ansible_facts.services.nginx is defined
        and ansible_facts.services.nginx.state == 'running'
...
